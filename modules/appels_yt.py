from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
import pandas as pd
import isodate
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv


# Charger les variables d'environnement (cl√© API)
load_dotenv()

# Gestion de la cl√© API YouTube
API_KEY = os.getenv("YOUTUBE_API_KEY") # .env
print(f"Cl√© API charg√©e : {API_KEY}")  # DEBUG : V√©rifier la cl√© API
# V√©rifier si la cl√© API est bien charg√©e
if not API_KEY:
    # raise interrompt tout le programme si il y a une erreur.
    raise ValueError("Erreur : La cl√© API YouTube n'est pas d√©finie. V√©rifie ton fichier .env.")

# Cr√©er le service API (√† faire une seule fois)
youtube = build('youtube', 'v3', developerKey=API_KEY)

# Fonction pour qualifier une destination via YouTube (ben√©dicte)
def qualif_destination(search_query):
    """
    Recherche des vid√©os YouTube sur une destination et retourne un DataFrame Pandas.
    """
    # Calculer la date limite pour les 10 derni√®res ann√©es
    date_limit = (datetime.now() - timedelta(days=10 * 365)).isoformat("T") + "Z"

    # Initialiser une liste pour stocker les r√©sultats
    video_data = []

    # Param√®tres pour la pagination
    max_results_per_page = 50  # 50 est le Maximum permis par l'API
    # Nombre de vid√©os souhait√©es maximum
    total_videos_needed = 20  # Nombre total de vid√©os souhait√©es
    next_page_token = None

    try:
        # √âtape 1 : Effectuer des recherches pagin√©es et collecter les donn√©es
        while True:
            # Requ√™te de recherche
            search_response = youtube.search().list(
                part='snippet',
                q=search_query,
                type='video',
                maxResults=max_results_per_page,
                #gl="US",  # R√©sultats localis√©s aux √âtats-Unis
                regionCode="US",  # Cible les vid√©os des √âtats-Unis
                relevanceLanguage = "en",  # Langue anglaise
                publishedAfter=date_limit,  # Vid√©os publi√©es au cours des 10 derni√®res ann√©es
                pageToken=next_page_token
            ).execute()

            # on remplace la boucle for par une liste en compr√©hension, plus rapide 
            # qui permet de r√©cup√©rer les videoId et de les stocker dans une liste
            video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
            
            # on gere le doublon eventuel on transforme la liste en set puis on la remet en liste
            video_ids = list(set(video_ids))
            
            # Aucune vid√©o trouv√©e, on arr√™te la recherche
            if not video_ids:
                break  
            
            # √âtape 2 : R√©cup√©ration des d√©tails des vid√©os en une seule requ√™te (optimisation)
            video_response = youtube.videos().list(
                part='snippet,contentDetails,statistics',
                id=','.join(video_ids)  # Convertir la liste en une cha√Æne s√©par√©e par des virgules
            ).execute()           
            
            for video in video_response.get('items', []):   
                # R√©cup√©rer les d√©tails de chaque vid√©o
                snippet = video.get('snippet', {})
                statistics = video.get('statistics', {})
                content_details = video.get('contentDetails', {})

                # Dur√©e (convertie en secondes pour comparaison)
                raw_duration = content_details.get('duration', 'PT0S')
                duration_seconds = int(isodate.parse_duration(raw_duration).total_seconds())

                # Crit√®res de filtre
                if (int(statistics.get('viewCount', 0)) >= 1000 and
                        int(statistics.get('commentCount', 0)) >= 20 and
                        240 <= duration_seconds <= 1200):  # 4 minutes √† 20 minutes
                    # Ajouter les donn√©es filtr√©es dans la liste
                    video_data.append({
                        'Video_ID': video['id'],
                        'Title': snippet.get('title', 'Non disponible'),
                        'Description': snippet.get('description', 'Non disponible'),
                        'Published At': snippet.get('publishedAt', 'Non disponible'),
                        'Duration (Seconds)': duration_seconds,
                        'View Count': statistics.get('viewCount', 0),
                        'Like Count': statistics.get('likeCount', 0),
                        'Comment Count': statistics.get('commentCount', 0),
                        'URL': f"https://www.youtube.com/watch?v={video['id']}",
                        'Channel ID': snippet.get('channelId'),
                    })
                    if len(video_data) >= total_videos_needed:
                        break  # Stopper la boucle d√®s qu'on a le nombre requis

            # Pr√©parer la page suivante a condition que nous n'avons pas atteint 
            # le nombre total de vid√©os souhait√©es
            if len(video_data) >= total_videos_needed or 'nextPageToken' not in search_response:
                break  # Sortir de la boucle si on a assez de vid√©os ou plus de pages

            # Mettre √† jour le token pour la prochaine page (si disponible)
            next_page_token = search_response.get('nextPageToken')

            # V√©rifier si la page suivante existe bien
            if not next_page_token:
                break  # Plus de pages disponibles, on arr√™te la recherche

    except Exception as e:
        print(f"Erreur lors de l'appel √† l'API YouTube : {e}")
        # Retourne un DataFrame vide en cas d'erreur et une liste portant les video_ids
        return pd.DataFrame(), []  

    # √âtape 3 : Convertir les r√©sultats en DataFrame Pandas
    df_destination = pd.DataFrame(video_data)
    
    # Retourner le DataFrame et la liste des identifiants des vid√©os    
    return df_destination, video_ids

# fonction de r√©cup√©ration des transcripts youtube (mireille)
def recup_transcript(video_ids):
    #cr√©ation d'un dictionnaire pour stocker les transcripts
    transcripts = {}
    #boucle pour r√©cup√©rer les transcripts de chaque vid√©o dans videos_ids
    for video_id in video_ids:
        try:
            #r√©cup√©ration du transcript d'une vid√©o, retourne une liste de dictionnaires
            transcript = YouTubeTranscriptApi.get_transcript(video_id)
            #on transforme la liste de dictionnaires en texte et on l'ajoute au dictionnaire transcripts
            # avec video_id comme cl√©
            transcripts[video_id] = " ".join([entry['text'] for entry in transcript])
        except Exception as e:
            #en cas d'erreur, on stocke le message d'erreur dans le dictionnaire (facilera le nettoyage)
            #on limite le retour de l'erreur √† 100 caract√®res
            transcripts[video_id] = f"Transcript non disponible : {str(e)[:100]}..."
    # Retourne tous les transcripts apr√®s avoir parcouru toutes les vid√©os
    return transcripts  






# Ce bloc de code permet d'√©viter que le script soit ex√©cut√© automatiquement lorsqu'il est 
# import√© dans un autre fichier.
# Il ne s'ex√©cute que si ce fichier est ex√©cut√© directement via `python recup_url_yt.py`.
if __name__ == "__main__":
    print("üîπ Test en mode standalone : ce script est ex√©cut√© directement.")
