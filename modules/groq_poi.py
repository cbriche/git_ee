from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate
import os
from dotenv import load_dotenv
load_dotenv()  # Charge les variables d'environnement depuis .env
from pprint import pprint

# R√©cup√©ration de la cl√© API Groq
GROQ_API_KEY = os.getenv("GROQ_API_KEY") # .env

from langchain_groq import ChatGroq

# Initialisation de llm et prompt en dehors de la fonction pour √©viter de les recr√©er √† chaque appel
llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0.4,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    model_kwargs={"max_completion_tokens":380},
    api_key=GROQ_API_KEY
)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Tu es un expert  voyages.\n"
                "Donne-moi une liste de 10 points d'int√©r√™t \n"
                "incontournables pour la destination suivante et  \n"
                "uniquement cette destination"
        ),
        ("human", "{search_query}"),
    ]
)

chain = prompt | llm


# Fonction de r√©sum√© d'un transcript YouTube
def trouver_poi(search_query) -> str:
    """Extraction poi avec LangChain."""
    if search_query is None:  # V√©rifie si transcript est None ou vide
        return "Transcript vide ou non disponible."  # Message par d√©faut si le transcript est invalide
    try:
        result = chain.invoke({"search_query": search_query})
        return result.content
        # ou bien return result['text'] if 'text' in result else "Aucun texte g√©n√©r√©."

    except Exception as e:
        return f"Erreur lors de la g√©n√©ration du r√©sum√© : {str(e)}"

# Ce bloc de code permet d'√©viter que le script soit ex√©cut√© automatiquement lorsqu'il est 
# import√© dans un autre fichier.
# Il ne s'ex√©cute que si ce fichier est ex√©cut√© directement via `python liste_video_yt.py`.
if __name__ == "__main__":
    print("üîπ Test en mode standalone : ce script est ex√©cut√© directement.")

